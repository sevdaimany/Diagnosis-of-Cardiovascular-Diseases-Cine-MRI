{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import configparser\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCM patient\n",
    "patient_cfg = configparser.ConfigParser()\n",
    "patient_cfg.read_string('[Info]\\n' + open('./ACDC/training/patient001/info.cfg').read())\n",
    "\n",
    "ed = patient_cfg.get('Info', 'ED')\n",
    "es = patient_cfg.get('Info', 'ES')\n",
    "group = patient_cfg.get('Info', 'Group')\n",
    "nbFrame = patient_cfg.get('Info', 'NbFrame')\n",
    "height = patient_cfg.get('Info', 'Height')\n",
    "weight = patient_cfg.get('Info', 'Weight')\n",
    "\n",
    "nii_file_path1 = \"./ACDC/training/patient001/patient001_frame01.nii.gz\"\n",
    "nii_file_path2 = \"./ACDC/training/patient001/patient001_frame01_gt.nii.gz\"\n",
    "\n",
    "nii_img1 = nib.load(nii_file_path1)\n",
    "nii_data1 = nii_img1.get_fdata()\n",
    "\n",
    "nii_img2 = nib.load(nii_file_path2)\n",
    "nii_data2 = nii_img2.get_fdata()\n",
    "\n",
    "# Display slices of both NIfTI images side by side using matplotlib\n",
    "num_slices = nii_data1.shape[2]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 5 * num_slices))\n",
    "\n",
    "for slice_idx in range(num_slices):\n",
    "    plt.subplot(num_slices, 2, 2*slice_idx + 1)\n",
    "    plt.imshow(nii_data1[:, :, slice_idx], cmap=\"gray\")\n",
    "    plt.title(f\"DCM - Slice {slice_idx}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(num_slices, 2, 2*slice_idx + 2)\n",
    "    plt.imshow(nii_data2[:, :, slice_idx], cmap=\"gray\")\n",
    "    plt.title(f\"GT - Slice {slice_idx}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "title = f\"Patient 1, ED: {ed}, ES: {es}, Group: {group}, NbFrame: {nbFrame}, Height: {height}, Weight: {weight}\"\n",
    "plt.suptitle(title)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mask_single(y):\n",
    "    \"\"\"\n",
    "    Given one masks with many classes create one mask per class\n",
    "    y: shape (w,h)\n",
    "    \"\"\"\n",
    "    mask = np.zeros((4, y.shape[0], y.shape[1]))\n",
    "    mask[0, :, :] = np.where(y == 0, 1, 0)\n",
    "    mask[1, :, :] = np.where(y == 1, 1, 0)\n",
    "    mask[2, :, :] = np.where(y == 2, 1, 0)\n",
    "    mask[3, :, :] = np.where(y == 3, 1, 0)\n",
    "\n",
    "    return mask\n",
    "\n",
    "def get_images(img_path, input_size=(224,224,1)):\n",
    "    all_imgs = []\n",
    "    img = nib.load(img_path).get_fdata()\n",
    "    for idx in range(img.shape[2]):\n",
    "        i = cv2.resize(img[:,:,idx], (input_size[0], input_size[1]), interpolation=cv2.INTER_NEAREST)\n",
    "        # cv2.imwrite(f\"raw_{idx}.png\", img[:,:,idx].astype(\"float32\"))\n",
    "        all_imgs.append(i)\n",
    "    all_imgs = np.expand_dims(all_imgs, axis=3)\n",
    "    return [all_imgs, torch.empty((all_imgs.shape[0], all_imgs.shape[1], all_imgs.shape[2], 1), dtype=torch.float32)]\n",
    "\n",
    "\n",
    "def visualize(image_raw,mask):\n",
    "    \"\"\"\n",
    "    iamge_raw:gray image with shape [width,height,1]\n",
    "    mask: segment mask image with shape [num_class,width,height]\n",
    "    this function return an image using multi color to visualize masks in raw image\n",
    "    \"\"\"\n",
    "    # Convert grayscale image to RGB\n",
    "    image = cv2.cvtColor(image_raw, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "#     image = image_raw\n",
    "#     mask = mask.numpy()\n",
    "\n",
    "    # Get the number of classes (i.e. channels) in the mask\n",
    "    num_class = mask.shape[0]\n",
    "\n",
    "\n",
    "    # Define colors for each class (using a simple color map)\n",
    "    colors = []\n",
    "    for i in range(1, num_class):  # skip first class (background)\n",
    "        hue = int(i/float(num_class-1) * 179)\n",
    "        color = np.zeros((1, 1, 3), dtype=np.uint8)\n",
    "        color[0, 0, 0] = hue\n",
    "        color[0, 0, 1:] = 255\n",
    "        color = cv2.cvtColor(color, cv2.COLOR_HSV2RGB)\n",
    "        colors.append(color)\n",
    "\n",
    "    # Overlay each non-background class mask with a different color on the original image\n",
    "    for i in range(1, num_class):\n",
    "        class_mask = mask[i, :, :]\n",
    "        class_mask = np.repeat(class_mask[:, :, np.newaxis], 3, axis=2)\n",
    "        class_mask = class_mask.astype(image.dtype)\n",
    "        class_mask = class_mask * colors[i-1]\n",
    "        image = cv2.addWeighted(image, 1.0, class_mask, 0.5, 0.0)\n",
    "\n",
    "    return image\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    patient_id = 101\n",
    "    slice_id = 1\n",
    "    i = 0\n",
    "    scores = pd.DataFrame(columns=['patient_id', 'slice_id', 'dice_avg', 'dice_lv', 'dice_rv', 'dice_myo'])\n",
    "    \n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        y_pred = torch.argmax(outputs[2], axis=1)\n",
    "        mask = convert_mask_single(y_pred[0, :, :].cpu().numpy())\n",
    "        \n",
    "        # Visualize the input image, ground truth mask, and predicted mask\n",
    "        input_image = inputs[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        # convert into a single channel to visualize\n",
    "        ground_truth_mask = torch.argmax(targets[0], dim=0)\n",
    "        predicted_mask = y_pred.cpu().numpy().transpose(1, 2, 0)\n",
    "        mask_with_image = visualize(input_image, mask)\n",
    "        mask_with_image = (mask_with_image - mask_with_image.min()) / (mask_with_image.max()- mask_with_image.min()) *255\n",
    "#         cv2.imwrite('here.jpg',mask_with_image)    \n",
    "    \n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.title(\"Input Image\")\n",
    "        plt.imshow(input_image, cmap='gray')\n",
    "\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.title(\"Ground Truth Mask\")        \n",
    "        plt.imshow(ground_truth_mask.cpu(), cmap='gray')\n",
    "\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.title(\"Predicted Mask\")\n",
    "        plt.imshow(predicted_mask, cmap='gray')\n",
    "        \n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.title(\"Predicted Mask2\")\n",
    "        plt.imshow(mask_with_image.astype(np.uint8))\n",
    "        plt.show()\n",
    "#         i+=1\n",
    "#         if  i == 2:\n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image and its groud truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = \"./ACDC/training/patient002/patient002_frame01.nii.gz\"\n",
    "\n",
    "# input_size = (224, 224, 1)\n",
    "# all_imgs = []\n",
    "# img = nib.load(img_path).get_fdata()\n",
    "# for idx in range(img.shape[2]):\n",
    "    \n",
    "#     i = cv2.resize(img[:,:,idx], (input_size[0], input_size[1]), interpolation=cv2.INTER_NEAREST)\n",
    "#     all_imgs.append(i)\n",
    "# #     plt.title(\"Input Image\")\n",
    "# #     plt.imshow(i, cmap='gray')\n",
    "# #     plt.show()\n",
    "#     # cv2.imwrite(f\"raw_{idx}.png\", img[:,:,idx].astype(\"float32\"))\n",
    "\n",
    "\n",
    "# img_path = \"pred_feature_patient002_frame01.nii.gz\"\n",
    "img_path = \"./ACDC/training/patient002/patient002_frame01_sg.nii.gz\"\n",
    "input_size = (224, 224, 1)\n",
    "all_imgs_gt = []\n",
    "img = nib.load(img_path).get_fdata()\n",
    "for idx in range(img.shape[2]):\n",
    "    i = cv2.resize(img[:,:,idx], (input_size[0], input_size[1]), interpolation=cv2.INTER_NEAREST)\n",
    "    all_imgs_gt.append(i)\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.imshow(i, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    # cv2.imwrite(f\"raw_{idx}.png\", img[:,:,idx].astype(\"float32\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = \"pred_feature_patient002_frame01.nii.gz\"\n",
    "img_path = \"./ACDC/training/patient002/patient002_frame01_gt.nii.gz\"\n",
    "input_size = (224, 224, 1)\n",
    "all_imgs_gt = []\n",
    "img = nib.load(img_path).get_fdata()\n",
    "for idx in range(img.shape[2]):\n",
    "    i = cv2.resize(img[:,:,idx], (input_size[0], input_size[1]), interpolation=cv2.INTER_NEAREST)\n",
    "    all_imgs_gt.append(i)\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.imshow(i, cmap='gray')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With one .nii image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/fct.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import get_acdc,convert_masks\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import os\n",
    "\n",
    "img_path = \"./ACDC/testing/patient101/patient101_frame01.nii.gz\"   \n",
    "\n",
    "acdc_data= get_images(img_path, input_size=(224, 224, 1))\n",
    "acdc_data[1] = convert_masks(acdc_data[1])\n",
    "acdc_data[0] = np.transpose(acdc_data[0], (0, 3, 1, 2)) # for the channels\n",
    "acdc_data[1] = np.transpose(acdc_data[1], (0, 3, 1, 2)) # for the channels\n",
    "acdc_data[0] = torch.Tensor(acdc_data[0]) # convert to tensors\n",
    "acdc_data[1] = torch.Tensor(acdc_data[1]) # convert to tensors\n",
    "acdc_data = TensorDataset(acdc_data[0], acdc_data[1])\n",
    "test_loader = DataLoader(acdc_data, batch_size=1, num_workers=2)\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with all the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import get_acdc,convert_masks\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "acdc_data,_,_ = get_acdc(\"ACDC/testing\", input_size=(224, 224, 1))\n",
    "print(acdc_data[1].shape, acdc_data[0].shape)\n",
    "acdc_data[1] = convert_masks(acdc_data[1])\n",
    "acdc_data[0] = np.transpose(acdc_data[0], (0, 3, 1, 2)) # for the channels\n",
    "acdc_data[1] = np.transpose(acdc_data[1], (0, 3, 1, 2)) # for the channels\n",
    "acdc_data[0] = torch.Tensor(acdc_data[0]) # convert to tensors\n",
    "acdc_data[1] = torch.Tensor(acdc_data[1]) # convert to tensors\n",
    "acdc_data = TensorDataset(acdc_data[0], acdc_data[1])\n",
    "test_loader = DataLoader(acdc_data, batch_size=1, num_workers=2)\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = evaluate_model(model, test_loader)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save predicted images for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"./ACDC/testing/patient103/patient103_frame01.nii.gz\"\n",
    "\n",
    "input_size = (224, 224, 1)\n",
    "all_imgs_gt = []\n",
    "img = nib.load(img_path).get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    results = []\n",
    "    \n",
    "    \"\"\"\n",
    "    outputs[2],  torch.Size([1, 4, 224, 224])\n",
    "    y_pred,  torch.Size([1, 224, 224])\n",
    "    mask,  (4, 224, 224)\n",
    "    predicted_mask  (224, 224, 1)\n",
    "    \"\"\"\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        y_pred = torch.argmax(outputs[2], axis=1)\n",
    "        \n",
    "        results.append(y_pred[0,:,:].cpu().numpy())\n",
    "    return results\n",
    "        \n",
    "def get_images(img, input_size=(224,224,1)):\n",
    "    \"\"\"\n",
    "    given one .nii file and return all the frames in one list\n",
    "    \"\"\"\n",
    "    all_imgs = []\n",
    "    # img = nib.load(img).get_fdata()\n",
    "    # img = img.get_fdata()\n",
    "    for idx in range(img.shape[2]):\n",
    "        i = cv2.resize(img[:,:,idx], (input_size[0], input_size[1]), interpolation=cv2.INTER_NEAREST)\n",
    "        # cv2.imwrite(f\"raw_{idx}.png\", img[:,:,idx].astype(\"float32\"))\n",
    "        all_imgs.append(i)\n",
    "    all_imgs = np.expand_dims(all_imgs, axis=3)\n",
    "    return [all_imgs, torch.empty((all_imgs.shape[0], all_imgs.shape[1], all_imgs.shape[2], 1), dtype=torch.float32)]\n",
    " \n",
    "def predict(img, model):\n",
    "    \n",
    "    acdc_data= get_images(img, input_size=(224, 224, 1))\n",
    "    acdc_data[1] = convert_masks(acdc_data[1])\n",
    "    acdc_data[0] = np.transpose(acdc_data[0], (0, 3, 1, 2)) # for the channels\n",
    "    acdc_data[1] = np.transpose(acdc_data[1], (0, 3, 1, 2)) # for the channels\n",
    "    acdc_data[0] = torch.Tensor(acdc_data[0]) # convert to tensors\n",
    "    acdc_data[1] = torch.Tensor(acdc_data[1]) # convert to tensors\n",
    "    acdc_data = TensorDataset(acdc_data[0], acdc_data[1])\n",
    "    test_loader = DataLoader(acdc_data, batch_size=1, num_workers=2)\n",
    "    results = evaluate_model(model, test_loader)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/fct.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = predict(img, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array(res).transpose(1, 2, 0).astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine = np.eye(4)\n",
    "nifti_file = nib.Nifti1Image(res, affine)\n",
    "\n",
    "nib.save(nifti_file, \"predicted.nii.gz\") # Here you put the path + the extionsion 'nii' or 'nii.gz'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"predicted.nii.gz\"\n",
    "\n",
    "input_size = (224, 224, 1)\n",
    "all_imgs_gt = []\n",
    "img_mask = nib.load(img_path).get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(img_mask == res).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# model = torch.load('models/fct.model')\n",
    "\n",
    "# for root, directories, files in os.walk(\"ACDC/testing\"):\n",
    "#         for file in files:\n",
    "#             if \".gz\" and \"frame\" in file:\n",
    "#                 if \"_gt\" not in file:\n",
    "#                     img_path = root + \"/\" + file\n",
    "#                     out_path = root + \"/pred_\" + file\n",
    "#                     print(img_path)\n",
    "#                     img = nib.load(img_path).get_fdata()\n",
    "#                     data_loader = prepare_data(img)\n",
    "#                     result = model_output(model, dataloader)\n",
    "#                     affine = np.eye(4)\n",
    "#                     nifti_file = nib.Nifti1Image(result, affine)\n",
    "#                     nib.save(nifti_file, out_path) # Here you put the path + the extionsion 'nii' or 'nii.gz'\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
